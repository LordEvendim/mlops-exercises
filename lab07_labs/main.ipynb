{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11c6aca",
   "metadata": {},
   "source": [
    "### Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea27d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa73e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2027,  2007,  2074,  7103,  3797,  2004,  2026, 19208,  3554,\n",
      "          1016, 19548,  2217, 13001, 17425,  2083, 10630,  4137,  2576,  3392,\n",
      "          1014,  9534,  3370,  6597,  3392,  3130, 27137, 18140,  6133, 12009,\n",
      "          2106,  1016,  7371,  2098,  2083,  1045,  4177,  5306,  2098, 13661,\n",
      "          2103,  4301, 28177, 27588,  2106, 21187,  4454,  2067,  3806,  2083,\n",
      "         20190, 20205,  4866, 16215,  1016,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is some sample text to be tokenized. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\n",
    "\n",
    "inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c501b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No optimizations: 0.2849780583381653s\n",
      "eval(): 0.2348670768737793s (1.2133589012619095x speedup)\n",
      "eval() + no_grad(): 0.21505134105682372s (1.3251628980210108x speedup)\n",
      "eval() + inference_mode(): 0.1969376587867737s (1.4470470507964848x speedup)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def measure_inference_time(model, inputs):\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "\n",
    "model.train()\n",
    "time_no_opt = measure_inference_time(model, inputs)\n",
    "\n",
    "model.eval()\n",
    "time_eval = measure_inference_time(model, inputs)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    time_no_grad = measure_inference_time(model, inputs)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    time_inference_mode = measure_inference_time(model, inputs)\n",
    "\n",
    "print(f\"No optimizations: {time_no_opt}s\")\n",
    "print(f\"eval(): {time_eval}s ({time_no_opt/time_eval}x speedup)\")\n",
    "print(f\"eval() + no_grad(): {time_no_grad}s ({time_no_opt/time_no_grad}x speedup)\")\n",
    "print(\n",
    "    f\"eval() + inference_mode(): {time_inference_mode}s ({time_no_opt/time_inference_mode}x speedup)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032a6d0",
   "metadata": {},
   "source": [
    "We can see significant speed ups, altough the inference times are very small, each stage yields much better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094aeacf",
   "metadata": {},
   "source": [
    "### 2. PyTorch model compilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905d1b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation + warm-up time: 110.6852593421936s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_compile = time.time()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _ = compiled_model(**inputs)\n",
    "\n",
    "end_compile = time.time()\n",
    "result_time = end_compile - start_compile\n",
    "\n",
    "print(f\"Compilation + warm-up time: {result_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dde60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled model inference time: 0.25084442377090455s\n",
      "Speedup vs no optimizations: 1.1360749186851964x\n",
      "Speedup vs eval() + inference_mode(): 0.785098810753857x\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    time_compiled = measure_inference_time(compiled_model, inputs)\n",
    "\n",
    "print(f\"Compiled model inference time: {time_compiled}s\")\n",
    "print(f\"Speedup vs no optimizations: {time_no_opt / time_compiled}x\")\n",
    "print(f\"Speedup vs eval() + inference_mode(): {time_inference_mode / time_compiled}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50530ed",
   "metadata": {},
   "source": [
    "We can see the significant boost in the compiled model. Maybe I should consider doing it more often.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1bbcb",
   "metadata": {},
   "source": [
    "### 3. Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeea902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352a6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import quantize_dynamic\n",
    "import torch.nn as nn\n",
    "\n",
    "quantized_model = quantize_dynamic(model, qconfig_spec={nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e754e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 417.7300271987915 MB\n",
      "Quantized model size: 173.09671783447266 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "torch.save(model.state_dict(), \"model_original.pth\")\n",
    "torch.save(quantized_model.state_dict(), \"model_quantized.pth\")\n",
    "\n",
    "original_size = os.path.getsize(\"model_original.pth\")\n",
    "quantized_size = os.path.getsize(\"model_quantized.pth\")\n",
    "\n",
    "print(f\"Original model size: {original_size / (1024**2)} MB\")\n",
    "print(f\"Quantized model size: {quantized_size / (1024**2)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ecb49",
   "metadata": {},
   "source": [
    "Very interesting results, I was expecting something more like >1/2 of the inital size, but in this case the quantized model turned to be <1/2 of the initial size. Not sure why that's the case, but most likely some smart tricks performed under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b1442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.204613037109375s\n",
      "Quantized: 0.16590158224105836s\n",
      "Speedup: 1.233339877446546x\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "quantized_model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    time_original_cpu = measure_inference_time(model, inputs)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    time_quantized_cpu = measure_inference_time(quantized_model, inputs)\n",
    "\n",
    "print(f\"Original: {time_original_cpu}s\")\n",
    "print(f\"Quantized: {time_quantized_cpu}s\")\n",
    "print(f\"Speedup: {time_original_cpu / time_quantized_cpu}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc3da9",
   "metadata": {},
   "source": [
    "In this case I can say that this boost is for sure statistically significant and not just the measurment error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8027a",
   "metadata": {},
   "source": [
    "### 4. GPU optimization strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448fc624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 10 words\n",
      "Default: 0.1418938899040222s\n",
      "Max-autotune: 0.14797403573989867s (0.9589107250777168x speedup)\n",
      "Max-autotune-no-cudagraphs: 0.1509942603111267s (0.9397303553899798x speedup)\n",
      "\n",
      "Input size: 200 words\n",
      "Default: 1.998001790046692s\n",
      "Max-autotune: 1.9180034279823304s (1.0417091861762295x speedup)\n",
      "Max-autotune-no-cudagraphs: 1.9226209354400634s (1.0392073409880844x speedup)\n",
      "\n",
      "Input size: 1000 words\n",
      "Default: 1.9379348921775819s\n",
      "Max-autotune: 1.913768322467804s (1.0126277404772879x speedup)\n",
      "Max-autotune-no-cudagraphs: 1.9080325770378113s (1.01567180534527x speedup)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lorem_text import lorem\n",
    "import torch\n",
    "import time\n",
    "\n",
    "input_sizes = [10, 200, 1000]\n",
    "inputs_sized = [\n",
    "    tokenizer(lorem.words(words), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    for words in input_sizes\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for size, inputs in zip(input_sizes, inputs_sized):\n",
    "    print(f\"Input size: {size} words\")\n",
    "\n",
    "    compiled_default = torch.compile(model)\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_default(**inputs)\n",
    "        time_default = measure_inference_time(compiled_default, inputs)\n",
    "    print(f\"Default: {time_default}s\")\n",
    "\n",
    "    compiled_max_autotune = torch.compile(model, mode=\"max-autotune\")\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune(**inputs)\n",
    "        time_max_autotune = measure_inference_time(compiled_max_autotune, inputs)\n",
    "    speedup_autotune = time_default / time_max_autotune\n",
    "    print(f\"Max-autotune: {time_max_autotune}s ({speedup_autotune}x speedup)\")\n",
    "\n",
    "    compiled_no_cudagraphs = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_no_cudagraphs(**inputs)\n",
    "        time_no_cudagraphs = measure_inference_time(compiled_no_cudagraphs, inputs)\n",
    "    speedup_no_cudagraphs = time_default / time_no_cudagraphs\n",
    "    print(\n",
    "        f\"Max-autotune-no-cudagraphs: {time_no_cudagraphs}s ({speedup_no_cudagraphs}x speedup)\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8525b84",
   "metadata": {},
   "source": [
    "We can see little to no speed up and variations could be just the margin of error. From my research it could because for the exercise we moved the model to the CPU and most of the optimizations and speed ups could be visible on the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "605d2172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (8, 6)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c978db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of model_fp32 parameters: torch.float32\n",
      "Data type of data_fp32: torch.float32\n",
      "Data type of labels_fp32: torch.float32\n",
      "Loss fp32: 1.4398962259292603\n"
     ]
    }
   ],
   "source": [
    "model_fp32 = torch.nn.Linear(10, 1)\n",
    "data_fp32 = torch.randn(100, 10)\n",
    "labels_fp32 = torch.randn(100, 1)\n",
    "\n",
    "print(f\"Data type of model_fp32 parameters: {model_fp32.weight.dtype}\")\n",
    "print(f\"Data type of data_fp32: {data_fp32.dtype}\")\n",
    "print(f\"Data type of labels_fp32: {labels_fp32.dtype}\")\n",
    "\n",
    "output_fp32 = model_fp32(data_fp32)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_fp32 = loss_fn(output_fp32, labels_fp32)\n",
    "\n",
    "print(f\"Loss fp32: {loss_fp32.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a7bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of model_fp16 parameters: torch.float16\n",
      "Data type of data_fp16: torch.float16\n",
      "Data type of labels_fp16: torch.float16\n",
      "Loss fp16: 1.439892292022705\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = model_fp32.half()\n",
    "data_fp16 = data_fp32.half()\n",
    "labels_fp16 = labels_fp32.half()\n",
    "\n",
    "print(f\"Data type of model_fp16 parameters: {model_fp16.weight.dtype}\")\n",
    "print(f\"Data type of data_fp16: {data_fp16.dtype}\")\n",
    "print(f\"Data type of labels_fp16: {labels_fp16.dtype}\")\n",
    "\n",
    "output_fp16 = model_fp16(data_fp16)\n",
    "loss_fp16 = loss_fn(output_fp16.float(), labels_fp16.float())\n",
    "\n",
    "print(f\"Loss fp16: {loss_fp16.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa26146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32: 0.06270748615264893s\n",
      "float16: 0.050943562984466555s (1.2309206988873034x speedup)\n",
      "autocast: 0.05790738582611084s (1.082892713218867x speedup)\n"
     ]
    }
   ],
   "source": [
    "model_gpu = model.to(\"cuda\")\n",
    "model_gpu.eval()\n",
    "\n",
    "inputs_gpu = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    time_fp32 = measure_inference_time(model_gpu, inputs_gpu)\n",
    "print(f\"float32: {time_fp32}s\")\n",
    "\n",
    "model_fp16 = model_gpu.half()\n",
    "inputs_fp16 = inputs_gpu\n",
    "with torch.inference_mode():\n",
    "    time_fp16 = measure_inference_time(model_fp16, inputs_fp16)\n",
    "speedup_fp16 = time_fp32 / time_fp16\n",
    "print(f\"float16: {time_fp16}s ({speedup_fp16}x speedup)\")\n",
    "\n",
    "model_gpu = model_gpu.float()\n",
    "\n",
    "\n",
    "def measure_inference_time_autocast(model, inputs):\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            _ = model(**inputs)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    time_autocast = measure_inference_time_autocast(model_gpu, inputs_gpu)\n",
    "speedup_autocast = time_fp32 / time_autocast\n",
    "print(f\"autocast: {time_autocast}s ({speedup_autocast}x speedup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082b544",
   "metadata": {},
   "source": [
    "Changing to float16 gives significant speed up, but the autocast only gives a bit more. Most likely autocast is partialy overlapping with modifications in the second step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bdb4f",
   "metadata": {},
   "source": [
    "### 5. ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "model_cpu = model.eval().cpu()\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff7486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7d03d590f350>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "ort.InferenceSession(\"model.onnx\", sess_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold start\n",
      "Online optimization: 1.011584758758545s\n",
      "Offline optimization: 0.8138351440429688s\n",
      "Speedup: 1.2429848553027534x\n",
      "\n",
      "Inference time\n",
      "Online optimization: 0.07272726774215699s\n",
      "Offline optimization: 0.07199683189392089s\n",
      "Speedup: 1.0101453887486647x\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "# ONLINE\n",
    "start_online = time.time()\n",
    "sess_options_online = ort.SessionOptions()\n",
    "sess_options_online.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "ort_session_online = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=sess_options_online, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "end_online = time.time()\n",
    "cold_start_online = end_online - start_online\n",
    "\n",
    "# OFFLINE\n",
    "start_offline = time.time()\n",
    "sess_options_offline = ort.SessionOptions()\n",
    "sess_options_offline.graph_optimization_level = (\n",
    "    ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    ")\n",
    "ort_session_offline = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\",\n",
    "    sess_options=sess_options_offline,\n",
    "    providers=[\"CPUExecutionProvider\"],\n",
    ")\n",
    "end_offline = time.time()\n",
    "cold_start_offline = end_offline - start_offline\n",
    "\n",
    "print(\"Cold start\")\n",
    "print(f\"Online optimization: {cold_start_online}s\")\n",
    "print(f\"Offline optimization: {cold_start_offline}s\")\n",
    "print(f\"Speedup: {cold_start_online / cold_start_offline}x\")\n",
    "print()\n",
    "\n",
    "\n",
    "def measure_onnx_inference_time(session, inputs):\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        _ = session.run(None, inputs)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "\n",
    "time_online = measure_onnx_inference_time(ort_session_online, inputs_onnx)\n",
    "time_offline = measure_onnx_inference_time(ort_session_offline, inputs_onnx)\n",
    "\n",
    "print(\"Inference time\")\n",
    "print(f\"Online optimization: {time_online}s\")\n",
    "print(f\"Offline optimization: {time_offline}s\")\n",
    "print(f\"Speedup: {time_online / time_offline}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07e2c9",
   "metadata": {},
   "source": [
    "For sure we can see that the offline optimization has a better cold start, but the average inference time, yields similar restuls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41188467",
   "metadata": {},
   "source": [
    "**Docker exercise**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9588853",
   "metadata": {},
   "source": [
    "Image sizes:\n",
    "Torch compiled: 7850 MB\n",
    "ONNX 933 MB\n",
    "\n",
    "Yes, although the exercise is supposed to only use CPU, I could have used the CPU only PyTorch version like we did on one of the previous labs. But I think installing CUDA is more closer to something we would see on the production env. This shows huge storage size differences, mostly because when using ONNX we don't have to install PyTorch and all CUDA libraries that come with it.\n",
    "\n",
    "Average inference time:\n",
    "Torch compiled : 0.0734s\n",
    "ONNX: 0.0478s\n",
    "\n",
    "We can see the significant speed up in the inference time. I'm really surprised in the speed up. I'm only wondering if the experiment on the GPU would yield similar results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-07 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
